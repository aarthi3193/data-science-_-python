{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "• We first model data with simple models and analyze data for errors. \n",
    "• These errors signify data points that are difficult to fit by a simple model. \n",
    "• Then for later models, we particularly focus on those hard to fit data to get them right. \n",
    "• In the end, we combine all the predictors by giving some weights to each predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Sequential learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "> Fit a decision tree on data\n",
    "\n",
    "> calculate error residuals. Actual target value, minus predicted value\n",
    "[e1= y - y_predicted1 ]\n",
    "\n",
    "> Fit a new model on the error residuals as target variable with same input variables\n",
    "[e1_predicted]\n",
    "\n",
    "> Add the predicted residuals to the previous predictions\n",
    "[y_predicted2 = y_predicted1+ e1_predicted]\n",
    "\n",
    "> Fit another model on the residuals that is still left [e2 = y-y_predicted2]\n",
    "Repeat steps 2 to 5 until it starts overfitting or the sum of the residuals become constant\n",
    "\n",
    "> Overfitting can be controlled by consistently checking accuracy on validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1. fit a decision tree - (y_predicted1)\n",
    "\n",
    "2. calculate error residuals - e1 = y-y_predicted1 (pred on the trained model)\n",
    "\n",
    "3. fit a new model on the eror residuals (e1) as target (response variable) with the same input varibales - (e1_predicted)\n",
    "\n",
    "4. y_predicted2 = y_predicted1 + e1_predicted\n",
    "\n",
    "5. e2 = y- y_predicted2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. fit a new model on the eror residuals (e2) as target (response variable) with the same input varibales - (e2_predicted)\n",
    "\n",
    "4. y_predicted3 = y_predicted2 + e2_predicted\n",
    "\n",
    "5. e3 = y- y_predicted3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. fit a new model on the eror residuals (e3) as target (response variable) with the same input varibales - (e3_predicted)\n",
    "\n",
    "4. y_predicted4 = y_predicted3 + e3_predicted\n",
    "\n",
    "5. e4 = y- y_predicted4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adaboost\n",
    "\n",
    "gradient boosting machines\n",
    "\n",
    "xgboost - stable, training time (distributed, parallel, heuristics), avai;able on almost all platforms, mature (less bugs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regularization - add a penalty term to your cost function\n",
    "\n",
    "Linear Regression - Simple and Multiple linear regression\n",
    "\n",
    "Ridge  - L2 regularization - square of the weights\n",
    "\n",
    "Lasso - L1 regularization - weights\n",
    "\n",
    "Elastic net - alpha "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Logistic regression \n",
    "\n",
    "LG - regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Random forest\n",
    "\n",
    "Distributed random forest (parallel learners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgboost hyperparams\n",
    "\n",
    "> General params\n",
    "> Booster params\n",
    "> Learning params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Differnt ways of tuning hyperparameters\n",
    "\n",
    "* Random search - number of iterations - randomly select params in the search space - best parameters (sklearn)\n",
    "* Grid search - \n",
    "(sklearn)\n",
    "\n",
    "* Optimization - Bayesian optimization - hyperopt / sigopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
