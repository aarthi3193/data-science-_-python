{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CART - gini index \n",
    "\n",
    "ID3 - IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Entropy - characterises the impurity of arbitrary collection of samples\n",
    "\n",
    "Information gain is the measure of the difference in entropy from before and after the set S is split on an attribute A. In other words, how much uncertainty in S was reduced after splitting set S on attribute A\n",
    "\n",
    "IG(A, S) = H(S) - Summation(p(t)H(t))\n",
    "\n",
    "H(S) - Entropy of set S\n",
    "p(t) - proportion of the number of elements in t to the number of elements in set S\n",
    "H(t) - Entropy of subset t\n",
    "\n",
    "IG can be calculated for each remaining attribute \n",
    "The attribute with the largest information gain is used to split the set S on this iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1.compute the entropy for data-set\n",
    "2.for every attribute/feature:\n",
    "       1.calculate entropy for all categorical values\n",
    "       2.take average information entropy for the current attribute\n",
    "       3.calculate gain for the current attribute\n",
    "3. pick the highest gain attribute.\n",
    "4. Repeat until we get the tree we desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.compute the gini index for data-set\n",
    "2.for every attribute/feature:\n",
    "       1.calculate gini index for all categorical values\n",
    "       2.take average information entropy for the current attribute \n",
    "       3.calculate the gini gain\n",
    "3. pick the best gini gain attribute.\n",
    "4. Repeat until we get the tree we desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Underfitting\n",
    "\n",
    "Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Pruning - process of adjusting decision tree to minimize misclassification error is known as pruning\n",
    "\n",
    "2 ways\n",
    "> Prepruning\n",
    "> Postpruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "> Prepruning is the halting of subtree construction at some node after checking some measures\n",
    "> If partitioning the tuple at a node would result in a split that falls below a prespecified threshold, then pruning is done\n",
    "> Early stopping- Pre-pruning may stop the growth process prematurely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "> Grow decision tree to its entirety.\n",
    "> Trim the nodes of the decision tree in a bottom-up fashion.Postpruning is done by replacing the node with leaf\n",
    "> If error improves after trimming, replace sub- tree by a leaf node.\n",
    "> Criteria: If error of parent is lesser than its child then prune the tree else not . \n",
    "    i.e if Parent (error)< Child(error) then “Prune” else don’t Prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Resampling techniques\n",
    "\n",
    "with replacement\n",
    "\n",
    "without replacement\n",
    "\n",
    "bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "High Bias\n",
    "\n",
    "Low Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Problem is high variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "High Bias\n",
    "\n",
    "To reduce variance -  c0mbining multiple trees"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
