{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://machinelearningmastery.com/metrics-evaluate-machine-learning-algorithms-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluation metrics designed for comparing continuous values.\n",
    "\n",
    "\n",
    "Three common evaluation metrics for regression problems:\n",
    "\n",
    "\n",
    "Mean Absolute Error (MAE) is the mean of the absolute value of the errors:\n",
    "    \n",
    "Mean Squared Error (MSE) is the mean of the squared errors:\n",
    "    \n",
    "Root Mean Squared Error (RMSE) is the square root of the mean of the squared errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example true and predicted response values\n",
    "true = [10, 7, 5, 5]\n",
    "pred = [8, 6, 5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 2.0\n",
      "MSE: 7.5\n",
      "RMSE: 2.7386127875258306\n"
     ]
    }
   ],
   "source": [
    "# calculate these metrics by hand\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "print('MAE:', metrics.mean_absolute_error(true, pred))\n",
    "print('MSE:', metrics.mean_squared_error(true, pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(true, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAE is the easiest to understand, because it's the average error.\n",
    "MSE is more popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world.\n",
    "RMSE is even more popular than MSE, because RMSE is interpretable in the \"y\" units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Additional example, to demonstrate how MSE/RMSE punish larger errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 2.0\n",
      "MSE: 16.0\n",
      "RMSE: 4.0\n"
     ]
    }
   ],
   "source": [
    "# same true values as above\n",
    "true = [10, 7, 5, 5]\n",
    "\n",
    "# new set of predicted values\n",
    "pred = [10, 7, 5, 13]\n",
    "\n",
    "#pred = [10,6,4,3]\n",
    "\n",
    "# MAE is the same as before\n",
    "print('MAE:', metrics.mean_absolute_error(true, pred))\n",
    "\n",
    "# MSE and RMSE are larger than before\n",
    "print('MSE:', metrics.mean_squared_error(true, pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(true, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refer to excel sheet - for examples on differences between MAE and RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classification Accuracy.\n",
    "Logarithmic Loss.\n",
    "Area Under ROC Curve.\n",
    "Confusion Matrix.\n",
    "Classification Report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "> Receiver Operating Characteristic area under curve (ROC AUC)\n",
    "> Precision Recall area under curve (PR AUC) - (Class Imbalance Problems)\n",
    "\n",
    "Confusion Matrix\n",
    "\n",
    "AUC\n",
    "\n",
    "TPR(y) vs FPR(x)\n",
    "(sensitivity) vs (1-specificity)\n",
    "\n",
    "Precision=TP / TP + FP (Spam classification example)\n",
    "Recall(Sensitivity / TPR ) = TP / TP + FN (Medical test classification example)\n",
    "Specificity - TN / FP + TN\n",
    "\n",
    "PR \n",
    "Precision vs Recall\n",
    "\n",
    "TPR - \n",
    "FPR - \n",
    "\n",
    "F1 score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC of a classifier is equal to the probability that the classifier will rank a randomly chosen positive example \n",
    "higher than a randomly chosen negative example \n",
    "\n",
    "* the AUC score only depends on how well you well you can separate the two classes. \n",
    "In practice, this means that only the order of your predictions matter,\n",
    "\n",
    "* As a result of this, any rescaling done to your model's output probabilities \n",
    "will have no effect on your score. In some other competitions, adding a constant or multiplier to your predictions to rescale it to the distribution can help but that doesn't apply here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "> Since PR does not account for true negatives (as TN is not a component of either Precision or Recall), \n",
    "> or there are many more negatives than positives (a characteristic of class imbalance problem), use PR. \n",
    "> If not, use ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import log\n",
    "\n",
    "def logloss(true_label, predicted, eps=1e-15):\n",
    "  p = np.clip(predicted, eps, 1 - eps)\n",
    "  if true_label == 1:\n",
    "    return -log(p)\n",
    "  else:\n",
    "    return -log(1 - p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471805599453"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logloss(1, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10536051565782628"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logloss(0, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10536051565782628"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logloss(1, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3025850929940455"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logloss(1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.538776394910684"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logloss(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.992007221626415e-16"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logloss(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05129329438755058"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logloss(1, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "log_loss(true_label, predicted, eps=1e-15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21616187468057912"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss([\"spam\", \"ham\", \"ham\", \"spam\"],  \n",
    "...          [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
